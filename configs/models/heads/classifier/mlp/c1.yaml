hidden_ratio: 4
dropout: 0
norm: LayerNorm
activation: SquaredReLU
head_init_std: null